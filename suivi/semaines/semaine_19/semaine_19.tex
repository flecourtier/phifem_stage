\section{Semaine 19 : 12/06/2023 - 16/06/2023}
\graphicspath{{semaines/semaine_19/images/}}

\begin{abstract}
	Cette semaine, on a commencé par essayer de comprendre pourquoi le kernel se bloquait. On a tenté de résoudre un problème de correction sur une solution analytique avec nb\_vert=300 et en degré 10 et il n'y a pas eut de soucis. C'est pourquoi, je ne comprends pas pourquoi on a cette erreur !
	
	Comme on ne peut pas faire du haut degré avec Legendre, j'ai testé avec des plus petits degrés. J'ai remarqué que la projection de phi\_tild sur notre espace de fonction V\_ex pour calculer la norme L2 d'erreur prend beaucoup de temps. Avec un degré 5, on est sur environ 15s (que l'on doit multiplier par nb\_data=100 et nb\_epochs=8). Par la suite, j'ai eut un autre problème avec la correction par multiplication. En prenant P=4 polynômes de Legendre, j'obtiens exactement les mêmes erreurs pour $\tilde{\phi}$ de degré 3 ou 5. En prenant P=6 et un degré 5, mes erreurs sont moins bonnes qu'avec P=4.
	
	J'ai alors testé la convergence de la décomposition en série de Polynômes de Legendre, tout d'abord avec la méthode des trapèzes pour différents nombres de points $N$. Puis avec une méthode de quadrature de scipy (\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad}{scipy.integrate.quad}) qui prend une expression analytique de notre fonction en paramètre (et pas un ensemble de valeurs à des points connus). Il semblerait alors qu'avec la méthode de scipy, on est bien la convergence attendue.
	
	J'ai également corrigé la preuve sur l'inégalité pour le rehaussement avec FEM.
	
	Après une réunion mercredi avec Emmanuel et Michel, Emmanuel a proposé de tester les 2 idées suivantes. La première, tenter de corriger en $\mathbb{P}^{10}$ une solution analytique décomposée en série de polynômes de Legendre. La seconde, implémenter un réseau de neurones multi-perceptron avec $||w_\theta(x)\phi(x)-u(x)||_{H^1}$.
\end{abstract}

\subsection{Test pour le kernel}

Test pour voir si le kernel se bloque sur la solution analytique en $\mathbb{P}^{10}$ avec nb\_vert=300 :

\begin{minipage}{\linewidth}
	\centering
	\includegraphics[width=0.5\linewidth]{test_kernel.png}
\end{minipage}

\subsection{Test de convergence 1D (Legendre)}

Test de convergence de la décomposition en une série de polynôme de Legendre sur la solution analytique suivante
$$u_{ex}(x) = \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$
avec $\mu=0$ et $\sigma=1$.

On comparera pour différents nombres de points $N$, l'erreur max(|Y\_true-Y\_true\_reconsrtruct|) en faisant varier le nombre de polynômes de Legendre $P$. On a commencé par test en considérant une solution discrétisée en nos $N^2$ nœuds et en utilisant la méthode des trapèzes pour calculer les coefficients $\alpha_p$. Etant donné que les résultats n'étaient pas bons, on a testé avec une une méthode de quadrature de scipy (\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad}{scipy.integrate.quad}) qui prend une expression analytique de notre fonction en paramètre.

\begin{Rem}
	On notera que contrairement à la méthode des trapèzes, avec scipy le paramètre $N$ n'influence pas le calcul des coefficients. On calcule juste l'erreur en norme infini sur des solutions avec différents nombres de points.
\end{Rem}

Résultats obtenus avec la méthode des trapèzes :

\begin{minipage}{\linewidth}
	\centering
	\includegraphics[width=0.7\linewidth]{test_cvg_trapeze.png}
\end{minipage}

Résultats obtenus avec la méthode de scipy :

\begin{minipage}{\linewidth}
	\centering
	\includegraphics[width=0.7\linewidth]{test_cvg_scipy.png}
\end{minipage}

\subsection{Résultat Correction $\mathbb{P}^3$ et $\mathbb{P}^5$ (Legendre)}

\begin{minipage}{\linewidth}
	\centering
	\includegraphics[width=0.9\linewidth]{test_Corr_P3_P5.png}
\end{minipage}