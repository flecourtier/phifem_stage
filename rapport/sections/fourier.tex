\section{Fourier Neural Operator (FNO)} \label{FNO}
\graphicspath{{images/fourier}}


We will now introduce Fourier Neural Operators (FNO). For more information, please refer to the following articles \modif{ADD REF !}.

%\trad{En traitement d'image, on appelle image des tenseurs de taille $ni\times nj\times nk$ où $ni\times nj$ correspond à la résolution de l'image et $nk$ correspond à son nombre de channels. Par exemple, une image RGB (Red Green Blue) possède $nk=3$ channels. 
%On choisit ici de présenter le FNO comme un opérateur agissant sur des images discrètes. Les articles de référence le présente sous son aspect continu qui est un point de vue intéressant. En effet, c'est grâce à cette propriété que l'on peut l'entraîner/évaluer avec des images de différentes résolutions.}

In image treatment, we call image tensors of size $ni\times nj\times nk$, where $ni\times nj$ corresponds to the image resolution and $nk$ corresponds to its number of channels. For example, an RGB (Red Green Blue) image has $nk=3$ channels. 
We choose here to present the FNO as an operator acting on discrete images. Reference articles present it in its continuous aspect, which is an interesting point of view. Indeed, it is thanks to this property that it can be trained/evaluated with images of different resolutions.

The FNO methodology creates a relationship between two spaces from a finite collection of observed input-output pairs. \modif{Est-ce que je gardes cette phrase ?}

\subsection{Architecture of the FNO}

%\trad{La figure suivante \ref{FNO} décrit précisément l'architecture du FNO :}

The following figure (Figure \ref{FNO_schema}) describes the FNO architecture in detail:

\begin{figure}[H]
	\includegraphics[width=\linewidth]{"fno_schema.png"}
	\captionof{figure}{Architecture of the FNO}
	\label{FNO_schema}
\end{figure}

\trad{La structure du FNO est alors la suivante :}

\begin{equation*}
	G_\theta = P \circ \mathcal{H}_\theta^1 \circ \dots \circ \mathcal{H}_\theta^L \circ Q
\end{equation*}

\subsubsection{General structure of the FNO} \label{FNO.general}

%\trad{On va maintenant décrire un peu plus en détail la composition de ce schéma.
%\begin{enumerate}[label=\textbullet]
%	\item On part d'input X de shape (batch\_size, height, width, nb\_channels) avec batch\_size le nombre d'images que l'on traites en même temps, height et width les dimensions des images et nb\_channels le nombre de channels. On simplifiera par (bs,ni,nj,nk).
%	\item On effectue une transformation $P$ dans le but de passer à un espace avec plus de channels. Cette étape permet au réseau de construire une représentation suffisement riche des données.  On pourra par exemple effectuer une couche  Dense (aussi appelée fully-connected). 	
%	\item On applique ensuite $L$ couches de Fourier, notées $\mathcal{H}_\theta^l,\; l=1,\dots,L$, dont on détaillera les spécifications dans la section \modif{AJOUTER SECTION}.
%	\item On se ramène alors à la dimension cible en effectuant une transformation $Q$. Dans notre cas le nombre de channels en sortie est de 1.
%	\item On récupère alors la sortie du modèle $Y$ de shape (bs,ni,nj,1). 
%\end{enumerate}
%} 

We'll now describe the composition of this scheme in a little more detail :
\begin{enumerate}[label=\textbullet]
	\item We start with input X of shape (batch\_size, height, width, nb\_channels) with batch\_size the number of images to be processed at the same time, height and width the dimensions of the images and nb\_channels the number of channels. Simplify by (bs,ni,nj,nk).
	\item We perform a $P$ transformation in order to move to a space with more channels. This step enables the network to build a sufficiently rich representation of the data.  For example, a Dense layer (also known as fully-connected) can be used. 	
	\item We then apply $L$ Fourier layers, noted $\mathcal{H}_\theta^l,\; l=1,\dots,L$, whose specifications will be detailed in Section \ref{FNO.fourierlayer}.
	\item We then return to the target dimension by performing a $Q$ transformation. In our case, the number of output channels is 1.
	\item We then obtain the output of the $Y$ model of shape (bs,ni,nj,1). 
\end{enumerate}

\modif{rajouter la valeur des paramètres dans un tableau : width,modes... }

\subsubsection{Fourier Layer structure} \label{FNO.fourierlayer}

\trad{Chaque couche de Fourier est composé de deux sous-couches :}

\begin{equation*}
	\tilde{Y}=\mathcal{H}_\theta^l(\tilde{X})=\sigma\left(\mathcal{C}_\theta^l(\tilde{X})+\mathcal{B}_\theta^l(\tilde{X})\right)
\end{equation*}

\trad{où 
\begin{enumerate}[label=\textbullet]
	\item $\tilde{X}$ correspond à l'entrée de la couche courante et $\tilde{Y}$ à la sortie.
	\item $\sigma$ est une fonction d'activation. Pour $l=1,\dots,L-1$, on prendra la fonction d'activation ReLU (Rectified linear unit) et pour $l=L$ on prendra la fonction d'activation GELU (Gaussian Error Linear Units).\modif{rajouter schéma + argument fcts d'activation}
	\item $\mathcal{C}_\theta^l$ est une couche de convolution où la convolution est faite par FFT
	\item $\mathcal{B}_\theta^l$ is the "bias-layer".
\end{enumerate}}

\paragraph{Convolution sublayer}

\trad{Chaque couche de convolution $\mathcal{C}_\theta^l$ contient un kernel $\hat{W}$ entraînable et effectue la transformation}

\begin{equation*}
	\mathcal{C}_\theta^l(X)=\mathcal{F}^{-1}(\mathcal{F}(X)\cdot\hat{W})
\end{equation*}

\trad{où $\mathcal{F}$ correspond à la transformée de Fourier discrète (DFT) en 2D sur une grille de résolution $ni\times nj$}

\paragraph{Bias subLayer}

\subsection{Application}

\trad{Dans notre cas, on souhaite apprendre au FNO à prédire des solutions d'EDP. Plus précisément, on souhaite que le réseau soit capable de prédire la solution à partir d'un terme source $f$. 
\modif{enrichissement des données, grilles régulières}}