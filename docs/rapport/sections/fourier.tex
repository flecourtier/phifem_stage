\section{Fourier Neural Operator (FNO)} \label{FNO}
\graphicspath{{images/fourier}}

We will now introduce Fourier Neural Operators (FNO), which belong to the category of so-called neural operator networks. Unlike standard neural networks, which learn using inputs and outputs of fixed dimensions, neural operators learn operators, which are mappings between spaces of functions. They can be evaluated at any data resolution without the need for retraining. As a result, they are widely used in PDE solving and constitute an active field of research. For more information, please refer to the following article \cite{li_fourier_2021,li_fourier_2022,li_neural_2020,li_physics-informed_2023}.

In image treatment, we call image tensors of size $ni\times nj\times nk$, where $ni\times nj$ corresponds to the image resolution and $nk$ corresponds to its number of channels. For example, an RGB (Red Green Blue) image has $nk=3$ channels. 
We choose here to present the FNO as an operator acting on discrete images. The reference article \cite{li_fourier_2021} present it in its continuous aspect, which is an interesting point of view. Indeed, it is thanks to this property that it can be trained/evaluated with images of different resolutions.

\begin{Rem}
	The FNO used was implemented by Vincent Vigon\footnote{Vincent Vigon: https://irma.math.unistra.fr/~vigon/} using Python's tensorflow library\footnote{Tensorflow: https://www.tensorflow.org/?hl=fr}.
	Furthermore, note that this report does not include a test of model parameter variation.
\end{Rem}

\subsection{Architecture of the FNO}

The following figure (Figure \ref{FNO_schema}) describes the FNO architecture in detail:

\begin{figure}[H]
	\includegraphics[width=0.9\linewidth]{"fno_schema.png"}
	\captionof{figure}{Architecture of the FNO.}
	\label{FNO_schema}
\end{figure}

The architecture of the FNO is as follows:

\begin{equation*}
	G_\theta = Q \circ \mathcal{H}_\theta^L \circ \dots \circ \mathcal{H}_\theta^1 \circ P
\end{equation*}

We will now describe the composition of the Figure \ref{FNO_schema} in a little more detail :
\begin{enumerate}[label=\textbullet]
	\item We start with input X of shape (batch\_size, height, width, nb\_channels) with batch\_size the number of images to be processed at the same time, height and width the dimensions of the images and nb\_channels the number of channels. Simplify by (bs,ni,nj,nk).
	\item We perform a $P$ transformation in order to move to a space with more channels. This step enables the network to build a sufficiently rich representation of the data.  For example, a Dense layer (also known as fully-connected) can be used. 	
	\item We then apply $L$ Fourier layers, noted $\mathcal{H}_\theta^l,\; l=1,\dots,L$, whose specifications will be detailed in Section \ref{FNO.fourierlayer}.
	\item We then return to the target dimension by performing a $Q$ transformation. In our case, the number of output channels is 1.
	\item We then obtain the output of the $Y$ model of shape (bs,ni,nj,1). 
\end{enumerate}

\begin{Rem}
	Note that the $P$ and $Q$ layers are in fact fully-connected multi-layer perceptrons, which means that they perform local transformations at each point, and therefore do not depend on the mesh resolution considered.
	
	Fourier layers are also independent of mesh resolution. Indeed, as we learn in Fourier space, the value of the Fourier modes does not depend on the mesh resolution.
	
	We deduce that the entire FNO does not depend on the mesh resolution.
\end{Rem}

\subsection{Fourier Layer structure} \label{FNO.fourierlayer}

Each Fourier layer is divided into two sub-layers:

\begin{equation*}
	\tilde{Y}=\mathcal{H}_\theta^l(\tilde{X})=\sigma\left(\mathcal{C}_\theta^l(\tilde{X})+\mathcal{B}_\theta^l(\tilde{X})\right)
\end{equation*}

where
\begin{enumerate}[label=\textbullet]
	\item $\tilde{X}$ corresponds to the input of the current layer and $\tilde{Y}$ to the output.
	\item $\sigma$ is an activation function. For $l=1,\dots,L-1$, we'll take the activation function ReLU (Rectified Linear Unit) and for $l=L$ we'll take the activation function GELU (Gaussian Error Linear Units).
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.3\linewidth]{"activation_functions.png"}
		\captionof{figure}{Activation functions used.}
	\end{figure}
	
	\item $\mathcal{C}_\theta^l$ is a convolution layer where convolution is performed by FFT (Fast Fourier Transform). For more details, see Section \ref{FNO.conv_sublayer}.
	\item $\mathcal{B}_\theta^l$ is the "bias-layer". For more details, see Section \ref{FNO.bias_sublayer}.
\end{enumerate}

\subsubsection{Convolution sublayer} \label{FNO.conv_sublayer}

Each $\mathcal{C}_\theta^l$ convolution layer contains a trainable kernel $\hat{W}$ and performs the transformation

\begin{equation*}
	\mathcal{C}_\theta^l(X)=\mathcal{F}^{-1}(\mathcal{F}(X)\cdot\hat{W})
\end{equation*}

where $\mathcal{F}$ corresponds to the 2D Discrete Fourier Transform (DFT) on a $ni\times nj$ resolution grid and
\begin{equation*}
	(Y\cdot\hat{W})_{ijk}=\sum_{k'}Y_{ijk'}\hat{W}_{ijk'}
\end{equation*}
In other words, this transformation is applied channel by channel.

\begin{Rem}
	An image is fundamentally a signal. Just as 1D signals show changes in amplitude (sound) over time, 2D signals show variations in intensity (light) over space. The Fourier transform allows us to move from the spatial or temporal domain into the frequency domain. In a sound signal (1D signal), low frequencies represent low-pitched sounds and high frequencies represent high-pitched sounds. In the case of an image (2D signal), low frequencies represent large homogeneous surfaces and blurred parts, while high frequencies represent contours, more generally abrupt changes in intensity and, finally, noise.
\end{Rem}

The 2D DFT is defined by :

\begin{equation*}
\mathcal{F}(X)_{ijk}=\frac{1}{ni}\frac{1}{nj}\sum_{i'=0}^{ni-1}\sum_{j'=0}^{nj-1}X_{i'j'k}e^{-2\sqrt{-1}\pi\left(\frac{ii'}{ni}+\frac{jj'}{nj}\right)}
\end{equation*}

The inverse of the 2D DFT is defined by :

\begin{equation*}
\mathcal{F}^{-1}(X)_{ijk}=\sum_{i'=0}^{ni-1}\sum_{j'=0}^{nj-1}X_{i'j'k}e^{2\sqrt{-1}\pi\left(\frac{ii'}{ni}+\frac{jj'}{nj}\right)}
\end{equation*}

We can easily show that $\mathcal{F}$ is the reciprocal function of $\mathcal{F}^{-1}$. We have 

\begin{align*}
	\mathcal{F}^{-1}(\mathcal{F}(X))_{ijk}&=\sum_{i'=0}^{ni-1}\sum_{j'=0}^{nj-1}\mathcal{F}(X)_{i'j'k}e^{2\sqrt{-1}\pi\left(\frac{ii'}{ni}+\frac{jj'}{nj}\right)} \\	
	&=\frac{1}{ni}\frac{1}{nj}\sum_{i'j'}\sum_{i''j''}X_{i''j''k}e^{-2\sqrt{-1}\pi\left(\frac{i'i''}{ni}+\frac{j'j''}{nj}\right)}e^{2\sqrt{-1}\pi\left(\frac{ii'}{ni}+\frac{jj'}{nj}\right)} \\
	&=\frac{1}{ni}\frac{1}{nj}\sum_{i''j''}X_{i''j''k}\sum_{i'j'}e^{2\sqrt{-1}\pi\frac{i'}{ni}(i-i'')}e^{2\sqrt{-1}\pi\frac{j'}{nj}(j-j'')}
\end{align*}

Let

\begin{equation*}
	S=\sum_{i'j'}e^{2\sqrt{-1}\pi\frac{i'}{ni}(i-i'')}e^{2\sqrt{-1}\pi\frac{j'}{nj}(j-j'')}
\end{equation*}

Thus
\begin{enumerate}[label=\textbullet]
	\item If $(i,j)=(i'',j'')$ : $S=\sum_{i',j'}1=ni\times nj$
	\item If $(i,j)\ne(i'',j'')$ : 
	\begin{align*}
		S&=\sum_{i'}\left(e^{\frac{2\sqrt{-1}\pi}{ni}(i-i'')}\right)^{i'}\sum_{j'}\left(e^{\frac{2\sqrt{-1}\pi}{nj}(j-j'')}\right)^{j'} \\
		&=\frac{1-\left(e^{\frac{2\sqrt{-1}\pi}{ni}(i-i'')}\right)^{ni}}{1-e^{\frac{2\sqrt{-1}\pi}{ni}(i-i'')}}\times \frac{1-\left(e^{\frac{2\sqrt{-1}\pi}{nj}(j-j'')}\right)^{nj}}{1-e^{\frac{2\sqrt{-1}\pi}{nj}(j-j'')}} \\
		&=\frac{1-e^{2\sqrt{-1}\pi(i-i'')}}{1-e^{\frac{2\sqrt{-1}\pi}{ni}(i-i'')}}\times \frac{1-e^{2\sqrt{-1}\pi(j-j'')}}{1-e^{\frac{2\sqrt{-1}\pi}{ni}(j-j'')}}=0
	\end{align*}
	as the sum of a geometric sequence.
\end{enumerate}

We deduce that
\begin{equation*}
	\mathcal{F}^{-1}(\mathcal{F}(X))_{ijk} = \frac{1}{ni}\frac{1}{nj} \times ni\times nj\times X_{ijk} = X_{ijk}
\end{equation*}

And finally $\mathcal{F}$ is the reciprocal function of $\mathcal{F}^{-1}$.

For more details about the Convolution sub-layer, see Section \ref{FNO.details_conv}.

\subsubsection{Bias sub-layer} \label{FNO.bias_sublayer}

The bias layer is a 2D convolution with a kernel size of 1. This means that it only performs matrix multiplication on the channels, but pixel by pixel. In other words, it mixes channels via a kernel, but does not allow interaction between pixels.

Precisely,

\begin{equation*}
	\mathcal{B}_\theta^l(X)_{ijk}=\sum_{k'}X_{ijk}W_{k'k}+B_k
\end{equation*}

\newpage

\subsection{Some details on the convolution sub-layer} \label{FNO.details_conv}

In this section, we will specify some details for the convolution layer.

\subsubsection{Border issues}

Let $W=\mathcal{F}^{-1}(\hat{W})$, we have :
\begin{equation*}
	\mathcal{C}_\theta^l(\tilde{X})=\mathcal{F}^{-1}\left(\mathcal{F}(X)\cdot\hat{W}\right)=\tilde{X}\star W
\end{equation*}
with
\begin{equation*}
	(\tilde{X}\star W)_{ij}=\sum_{i'j'}\tilde{X}_{i-i'[ni],j-j'[nj]}W_{i'j'}
\end{equation*}

In other words, multiplying in Fourier space is equivalent to performing a $\star$ circular convolution in real space. 

\begin{Rem}
	These modulo operations are only natural for periodic images, which is not our case. The
	discontinuity that appears when we periodize the image causes oscillations on the edges of the filtered	images. To limit this problem, we will apply a padding on the image which is the fact to extend the images by adding pixels all around, before performing the convolution. After the convolution, we restrict the image to partially erase the oscillations.
\end{Rem}

\subsubsection{FFT}

To speed up computations, we will use the FFT (Fast Fourier Transform). The FFT is a fast algorithm to compute the DFT. It is recursive : The transformation of a signal of size $N$ is make from the decomposition of two sub-signals of size $N/2$. The complexity of the FFT is $N\log(N)$ whereas the natural algorithm, which is a matrix multiplication, has a complexity of $N^2$.

\subsubsection{Real DFT}

In reality, we'll be using a specific implementation of FFT, called RFFT (Real Fast Fourier Transform). In fact, for $\mathcal{F}^{-1}(A)$ to be real if $A$ is a complex-valued matrix, it is necessary that A respects the Hermitian symmetry:
\begin{equation*}
	A_{i,nj-(j+1)} = \bar{A}_{i,j}
\end{equation*}

In our case, we want $\mathcal{C}_\theta^l(X)$ to be a real image, so $\mathcal{F}(X)\cdot\hat{W}$ must verify Hermitian-symmetry.

To do this, we only need to collect half of the Discrete Fourier Coefficients (DFC) and the other half will be deduced by Hermitian symmetry. More precisely, using the specific RFFT implementation, the DFCs are stored in a matrix of size $(ni,nj//2+1)$. Multiplication can then be performed by the $\hat{W}$ kernel, and when the inverse RFFT is performed, the DFCs will be automatically symmetrized. So the Hermitian symmetry of $\mathcal{F}(X)\cdot\hat{W}$ is verified and $\mathcal{C}_\theta^l(X)$ is indeed a real image.

To simplify, let's assume nk=1. Here is a diagram describing this idea:

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{"symetry_schema.png"}
	\captionof{figure}{RFFT with Hermitian-symmetry scheme.}
\end{figure}

\begin{Rem}
	In fact, we can check that $\mathcal{F}(X)$ satisfies Hermitian symmetry immediately.
\end{Rem}

\subsubsection{Low pass filter}

When we perform a DFT on an image, the DFCs related to high frequencies are in practice very low. This is why we can easily filter an image by ignoring these high frequencies, i.e. by truncating the high Fourier modes. In fact, eliminating the higher Fourier modes enables a kind of regularization that helps the generalization. So, in practice, it's sufficient to keep only the DFCs corresponding to low frequencies. Typically, for images of resolution $32\times 32$ to $128\times 128$, we can keep only the $20\times 20$ DFCs associated to low frequencies.

Here is a representation of this idea in 1D :

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{"fourier_low_pass_filter.png"}
	\captionof{figure}{Low pass filter.}
\end{figure}

\subsubsection{Global aspect of the FNO}

Classical Convolutional Neural Networks (CNN) use very small kernels (typically $3\times 3$). This operation only has a local effect, and it's the sequence of many convolutions that produces more global effects. 

In addition, CNNs often use max or mean-pooling layers, which process the image on several scales. Max-pooling (respectively mean-pooling) consists in slicing the image into small pieces of size $n\times n$, then choosing the pixel with the highest value (respectively the average of the pixels) in each of the small pieces. In most cases, $n=2$ is used, which divides the number of pixels by 4.

The FNO, on the other hand, uses a $\hat{W}$ frequency kernel and $W=\mathcal{F}^{-1}(\hat{W})$ has full support. For this reason, the effect is immediately non-local. As a result, we can use less layers and we don't need to use a pooling layer.

\subsection{Application} \label{FNO.application}

In our case, we want to use the FNO to predict the solutions of a PDE. As explained above, we'll train the FNO with a data set (sample of size $n_{data}$) generated by a $\phi$-FEM solver. We will then inject the output of our FNO into a new solver that can correct the solution, i.e. improve its accuracy. 

We are still trying to solve the Poisson problem with Dirichlet condition, defined by
\begin{equation*}
	\left\{\begin{aligned}
		-\Delta (\phi w)&=f, \; &&\text{on } \Omega, \\
		u&=g, \; &&\text{in } \Gamma,
	\end{aligned}\right.
\end{equation*}
with $u=\phi w$.

This problem can be approached in different ways.
For example, we may want to consider a case where the level-set function $\phi$ changes, as in the case where we want to solve the problem of the geometry of an organ at different time steps. In this case, we'll need to generate a $\{\phi_i\}_{i=1,\dots,n_{data}}$ collection of level-sets sufficiently representative of the possible variations of the levelset. In a more simple case, if our geometry is defined by an ellipse in a precise domain, the $\{\phi_i\}_{i=1,\dots,n_{data}}$ family will group $n_{data}$ ellipses whose parameters change, such as center or axes.

We may also want to solve the problem for a collection of source terms $\{f_i\}_{i=1,\dots,n_{data}}$. For example, this set could be a Gaussian set whose expected value and variance are varied. In the same idea, we might wish to vary the Dirichlet condition and thus create a collection $\{g_i\}_{i=1,\dots,n_{data}}$.

\begin{Rem}
	Note that we're working in a discrete way here, so for each $i$, the terms $f_i$, $g_i$ and $\phi_i$ are in fact 2D matrices of size $(ni,nj)$.
\end{Rem}

\begin{Rem}
	Note also that the FNO has less difficulty learning solutions that don't have a wide range of values. This is why the collection of $\{f_i\}$ that we'll be using in the following will in fact be the normalization of the previous collection :
	\begin{equation*}
		f_{i,norm} = \frac{f_i}{max_{j=1},\dots,n_{data} ||f_j||_{L^2(\mathcal{O})}}.
	\end{equation*}
\end{Rem}

Here are the steps that will be performed to train the FNO (Figure \ref{FNO_train_schema}):
\begin{enumerate}[label=\textbullet]
	\item We start by creating a dataset containing the level-set, source term and boundary condition collections, defined by
	\begin{equation*}
		X\_train = \{f_i,g_i,\phi_i\}_{i=1,\dots,n_{data}}.
	\end{equation*}
	\begin{Rem}
		Note that we can also consider the problem as homogeneous, in which case X\_train will only be generated from $f$ and $\phi$. We may also wish to fix the geometry, in which case the training sample X\_train will not contain the $\phi$ term.
	\end{Rem}
	We can then solve these $n_{data}$ problems using the $\phi$-FEM method, where the solution to each of them is defined by $u=\phi w$. We then define the training sample
	\begin{equation*}
		Y\_train=\{w_i\}_{i=1,\dots,n_{data}}
	\end{equation*}
	where $u_i = \phi_i w_i$ is the solution associated to the i-th problem of the X\_train sample, i.e. solution of
	\begin{equation*}
		\left\{\begin{aligned}
			-\Delta (\phi_i w_i)&=f_i, \; &&\text{on } \Omega, \\
			u_i&=g_i, \; &&\text{in } \Gamma.
		\end{aligned}\right.
	\end{equation*}
	\begin{Rem}
		Note that in practice, we have enriched the data by increasing the number of channels in the X\_train sample. In fact, for each problem $i$, we add to the sample the primitives of $f_i$ according to x and y, as well as the second primitives according to xx and yy. The X\_train sample is then of size $(n_{data},ni,nj,nk)$ with $nk=7$ the number of channels if we consider the 3 collections. The Y\_train sample is of size $(n_{data},ni,nj,1)$.
	\end{Rem}
	\item At this moment, we have the (X\_train,Y\_train) pair that will enable us to train our FNO. More precisely, we're looking to minimize a loss function on the model's $\theta$ parameters by using gradient descent. We'll choose the following loss function:
	\begin{equation*}
		loss_\theta = loss_\theta^{(0)} + loss_\theta^{(1)} + loss_\theta^{(2)}
	\end{equation*}
	with 
	\begin{align*}
		loss_\theta^{(0)} &= \frac{1}{n_{data}}\sum_{i=1}^{n_{data}} mse(\phi_iw_i-\phi_iw_{\theta,i}) \\
		loss_\theta^{(1)} &= \frac{1}{n_{data}}\sum_{i=1}^{n_{data}} mse(\nabla_x(\phi_iw_i)-\nabla_x(\phi_iw_{\theta,i}))+mse(\nabla_y(\phi_iw_i)-\nabla_y(\phi_iw_{\theta,i})) \\
		loss_\theta^{(2)} &= \frac{1}{n_{data}}\sum_{i=1}^{n_{data}} mse(\nabla_{xx}(\phi_iw_i)-\nabla_{xx}(\phi_iw_{\theta,i})) + mse(\nabla_{yy}(\phi_iw_i)-\nabla_{yy}(\phi_iw_{\theta,i}))
	\end{align*}
	with $w_i$ the $\phi$-FEM solution associated to the i-th problem considered (i.e. the i-th Y\_train sample data), $w_{\theta,i}$ the FNO prediction associated to the i-th problem, $\phi_i$ the i-th level-set and mse the "Mean Square Error" function defined by
	\begin{equation*}
		mse(A)=\frac{1}{ni}\frac{1}{nj}\sum_{i=0}^{ni-1}\sum_{j=0}^{nj-1}A_{i,j}^2.
	\end{equation*}
	\begin{Rem}
		Note that first and second derivatives according to x and y are calculated here by finite differences.
	\end{Rem}
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{"FNO_train_schema.png"}
	\captionof{figure}{Representation of the FNO training.}
	\label{FNO_train_schema}
\end{figure}

We can now proceed to the correction stage (Figure \ref{FNO_test_schema}), where we consider a type of correction defined in Section \ref{Corr.methods} (correction by adding, correction by multiplying or correction by multiplying on an enhanced problem). We can then consider a new test sample X\_test constructed in the same way as the training sample and defined by
\begin{equation*}
	X\_test = \{f^{test}_i,g^{test}_i,\phi^{test}_i\}_{i=1,\dots,n_{test}}.
\end{equation*}
We will then provide this sample as input to the FNO in order to obtain its prediction $w^{test}_{\theta,i}$ for each problem $i$ of the test sample (where $\theta$ corresponds to the parameters learned during training). We then construct $u^{test}_{\theta_i}=\phi_i w^{test}_{\theta,i}$ which will be given as input to one of the correction solvers. We will then obtain what we call the corrected solution.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{"FNO_test_schema.png"}
	\captionof{figure}{Representation of correction steps}
	\label{FNO_test_schema}
\end{figure}



